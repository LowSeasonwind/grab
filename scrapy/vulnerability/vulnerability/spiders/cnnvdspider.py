#coding=utf-8

import sys
import exceptions
import re
import uuid
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor
from scrapy.selector import Selector
from vulnerability.items import BugItem, midpro, vulnerability
if sys.getdefaultencoding() != 'utf-8':
    reload(sys)
    sys.setdefaultencoding('utf-8')

#cnnvd 中国国家安全信息漏洞库

class cnnvdspider(CrawlSpider):

    download_delay = 2
    retry_times = 5
    name = 'cnnvd'
    start_urls = ['http://www.cnnvd.org.cn/vulnerability?&p=4055']
    #start_urls = ['http://www.cnnvd.org.cn/vulnerability?&p=3209']
    allowed_domains = ['www.cnnvd.org.cn']

    rules = (Rule(LxmlLinkExtractor(allow=("",),
                restrict_xpaths=("//a[text()=\'"+u'下一页'+"\']",))),
             Rule(LxmlLinkExtractor(allow=("/vulnerability/show/cv_cnnvdid/",),
                restrict_xpaths=("//div[@class='dispage']/../table",)),
                  callback='parse_item')
             )

    def parse_item(self, response):
        try:
            sel = Selector(response)
            item = BugItem()
            vol = vulnerability()
            vol['vulName'] = ''.join( sel.xpath("//div[@class='cont_details']/table/\
                            tr[1]/td[2]/text()").extract())
            vol['cnnvd'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[2]/td[2]/text()").extract())
            vol['releaseTime'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[3]/td[2]/a/text()").extract())
            vol['updataTime'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[4]/td[2]/a/text()").extract())
            vol['vulType'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[6]/td[2]/a/text()").extract())
            vol['threadTypy'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[7]/td[2]/a/text()").extract())
            vol['cve'] = ''.join(sel.xpath("//div[@class='cont_details']/table/\
                            tr[8]/td[2]/a/text()").extract())
            vol['vulDescription'] = ''.join(sel.xpath("//table/tr[2]/td/div[@class='cont_details']/\
                            p/text()").extract())
            vol['vulAdvisory'] = sel.xpath("//table/tr[3]/td/div/p/a/text()").extract()
            vol['refWebsite'] = sel.xpath("//div[@id='top3']/table/tr\
                            /td/p/a/@href").extract()
            vol['cnvd'] = None
            item['vulnerability'] = vol

            # vol结束

            products = []
            if '暂无数据' not in sel.xpath("//table[2]/tr/td/div[@class='rht_cont']/table/tr/td/text()").extract():
                for line in sel.xpath("//table[2]/tr/td/div[@class='rht_cont']/table/tr/td/span/text()").extract():
                    values = line.split(':')
                    if values:
                        obj = midpro()
                        list = values[0].split(' ')
                        if list:
                            obj['vendor'] = list[0]
                            obj['name'] = list[1]
                        else:
                            obj['vendor'] = values[0]
                            obj['name'] = None
                        if re.search('\d+', values[-1]):
                            obj['version'] = values[-1]
                        else:
                            obj['version'] = None
                        obj['classify'] = None
                        obj['type'] = None
                        products.append(obj)
            item['product'] = products
            # product结束
            item['uuid'] = str(uuid.uuid1())
            item['kve'] = None
            item['is_ics'] = None
            item['CVSS'] = None
            yield item
        except exceptions:
            print '出错了，url:%s' %response.url

